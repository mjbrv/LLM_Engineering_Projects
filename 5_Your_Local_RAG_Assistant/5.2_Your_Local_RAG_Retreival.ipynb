{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local RAG Part 2: Query and Retrevival from Vector Database\n",
    "\n",
    "- Asking for user input/query\n",
    "- Vector Serach and retreival of the relevant docs\n",
    "- Modifications to include the used reference doc file(s) in the final responce\n",
    "- Puting all together in a Gradio UI\n",
    "\n",
    "ToDo:\n",
    "later you can add more models and the option to switch models in the UI!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from typing import List\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import chardet  # For detecting file encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\" #\"gpt-4o\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\"\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Want to keep costs ultra-low? Uncomment these lines:\n",
    "# OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "# CLAUDE_MODEL = \"claude-3-haiku-20240307\"\n",
    "\n",
    "db_name = \"mj_vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_check_db(db_name: str):\n",
    "    \"\"\"\n",
    "    Check if a Chroma database exists and load it, or inform if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        db_name (str): Name of the database directory\n",
    "        \n",
    "    Returns:\n",
    "        Chroma: The loaded vector store if it exists, None otherwise\n",
    "    \"\"\"\n",
    "    # Initialize the embedding function\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    # Check if the database directory exists\n",
    "    if os.path.exists(db_name):\n",
    "        try:\n",
    "            # Try to load the existing database\n",
    "            vectorstore = Chroma(\n",
    "                persist_directory=db_name,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "            print(f\"Successfully loaded database with {vectorstore._collection.count()} documents\")\n",
    "            return vectorstore\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading database: {str(e)}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Database '{db_name}' does not exist.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f3/8gs03snx5mgb4bxn_w3fxrqw0000gp/T/ipykernel_82781/1440877992.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded database with 81 documents\n"
     ]
    }
   ],
   "source": [
    "db_name = \"mj_vector_db\"\n",
    "vectorstore = load_or_check_db(db_name)\n",
    "\n",
    "if vectorstore is None:\n",
    "    print(\"Please create and populate the database first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Vector Store\n",
    "\n",
    "Let's take a minute to look at the documents and their embedding vectors to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_name'] for metadata in result['metadatas']]\n",
    "# colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "opacity": 0.8,
          "size": 5
         },
         "mode": "markers",
         "text": [
          "Type: all_summaries copy.docx<br>Text: 128. Day 1 - Fine-Tuning Large Language Models- From Inference to Training\n\nSummary\n\nIntroduction\n\nT...",
          "Type: all_summaries copy.docx<br>Text: Week 3: Explored Hugging Face, Pipelines Tokenizers, and models.\n\nWeek 4: Focused on selecting model...",
          "Type: all_summaries copy.docx<br>Text: Introduction to a New Problem\n\nNext, a commercial problem relating to an e-commerce company is intro...",
          "Type: all_summaries copy.docx<br>Text: 132. Day 1 - Evaluating LLM Performance- Model-Centric vs Business-Centric Metrics\n\nSummary \n\nData S...",
          "Type: all_summaries copy.docx<br>Text: Model Evaluation Metrics\n\nNext, the author delves into how model performance will be assessed — both...",
          "Type: all_summaries copy.docx<br>Text: Business-Centric Metrics\n\nThe speaker then discusses business-centric metrics, including average abs...",
          "Type: all_summaries copy.docx<br>Text: Strategy for LM Solution\n\nThe strategy vertically moves from identifying a business problem to deplo...",
          "Type: all_summaries copy.docx<br>Text: 5. Productionize: Following the successful deployment of a trained and functioning model, the next s...",
          "Type: all_summaries copy.docx<br>Text: Comparing, curating, and dividing data into training, validation, and test sections is paramount.\n\nS...",
          "Type: all_summaries copy.docx<br>Text: A five-step strategy to tackle commercial problems was outlined.\n\nExtensive work with data was done,...",
          "Type: all_summaries copy.docx<br>Text: Finally, Support Vector Regression, a technique designed for data grouping, will be used.\n\nThe effic...",
          "Type: all_summaries copy.docx<br>Text: Natural Language Processing \n\nThe text also highlights another set of imports related to natural lan...",
          "Type: all_summaries copy.docx<br>Text: Average Predictor: This model guesses that every item is priced at the average price of the training...",
          "Type: all_summaries.docx<br>Text: 128. Day 1 - Fine-Tuning Large Language Models- From Inference to Training\n\nSummary\n\nIntroduction\n\nT...",
          "Type: all_summaries.docx<br>Text: Week 3: Explored Hugging Face, Pipelines Tokenizers, and models.\n\nWeek 4: Focused on selecting model...",
          "Type: all_summaries.docx<br>Text: Introduction to a New Problem\n\nNext, a commercial problem relating to an e-commerce company is intro...",
          "Type: all_summaries.docx<br>Text: 132. Day 1 - Evaluating LLM Performance- Model-Centric vs Business-Centric Metrics\n\nSummary \n\nData S...",
          "Type: all_summaries.docx<br>Text: Model Evaluation Metrics\n\nNext, the author delves into how model performance will be assessed — both...",
          "Type: all_summaries.docx<br>Text: Business-Centric Metrics\n\nThe speaker then discusses business-centric metrics, including average abs...",
          "Type: all_summaries.docx<br>Text: Strategy for LM Solution\n\nThe strategy vertically moves from identifying a business problem to deplo...",
          "Type: all_summaries.docx<br>Text: 5. Productionize: Following the successful deployment of a trained and functioning model, the next s...",
          "Type: all_summaries.docx<br>Text: Comparing, curating, and dividing data into training, validation, and test sections is paramount.\n\nS...",
          "Type: all_summaries.docx<br>Text: A five-step strategy to tackle commercial problems was outlined.\n\nExtensive work with data was done,...",
          "Type: all_summaries.docx<br>Text: Finally, Support Vector Regression, a technique designed for data grouping, will be used.\n\nThe effic...",
          "Type: all_summaries.docx<br>Text: Natural Language Processing \n\nThe text also highlights another set of imports related to natural lan...",
          "Type: all_summaries.docx<br>Text: Average Predictor: This model guesses that every item is priced at the average price of the training...",
          "Type: all_summaries.docx<br>Text: Absolute Size of Llama 3.1 and its Implications\n\nThe author notes that even the 8 billion model is p...",
          "Type: all_summaries.docx<br>Text: Laura (Low Rank Adaptation)\n\nThe cornerstone of Laura is freezing the immense amount of weights to p...",
          "Type: all_summaries.docx<br>Text: The upcoming section will share more about 'quantization', another aspect to optimize the model.\n\n\n1...",
          "Type: all_summaries.docx<br>Text: Benefits of Quantization\n\nQuantization, a surprising yet effective method, works to maintain the sam...",
          "Type: all_summaries.docx<br>Text: Secondly, the process of lowering the precision is only applied to the base model. The Lora adapters...",
          "Type: all_summaries.docx<br>Text: Essential Hyperparameters\n\nThe three critical hyperparameters in Q Laura fine tuning involve:\n\nRank ...",
          "Type: all_summaries.docx<br>Text: Following an overview, we apply these hyperparameters in a practical session using Google Colab to b...",
          "Type: all_summaries.docx<br>Text: A Peft model from pre-trained is utilized. It is loaded using the base model as a reference, with th...",
          "Type: all_summaries.docx<br>Text: Parameter Calculation\n\nThe number of weights (parameters) across these adapters is calculated by mul...",
          "Type: all_summaries.docx<br>Text: The discussion concludes with a return to presentation slides for final thoughts.\n\n\nWeek 7\n\n169. Day...",
          "Type: all_summaries.docx<br>Text: original pre-trained version (foundation model)\n\nfine-tuned for chat purposes (instruct variant), wh...",
          "Type: all_summaries.docx<br>Text: Personal Insights\n\nIn my experiment, the base model slightly performed better than the instruct vari...",
          "Type: all_summaries.docx<br>Text: 2.52.5 Variant, the highest scorer.\n\nGemma, a 9 billion parameter model.\n\nMistral\n\nPhi2 from Microso...",
          "Type: all_summaries.docx<br>Text: Phi3, Gemma, and Meta Llama 3.1: These also perform well with 'instruct variants', providing a reali...",
          "Type: all_summaries.docx<br>Text: This selection process is a balance of investigating the leaderboard, learning about instruct varian...",
          "Type: all_summaries.docx<br>Text: 1. Target Modules\n\nThis is a subset of layers within the large architecture of a Transformer model s...",
          "Type: all_summaries.docx<br>Text: 3. Alpha\n\nAlpha is a scaling factor used in weight adjustment, where weight change equals α times tw...",
          "Type: all_summaries.docx<br>Text: 176. Day 3 - Understanding Epochs and Batch Sizes in Model Training\n\nSummary of Hyperparameters in M...",
          "Type: all_summaries.docx<br>Text: Enhanced performance by allowing faster data processing.\n\nDifferent variation of batches with each e...",
          "Type: all_summaries.docx<br>Text: Learning Rate\n\nLearning rate is a critical hyperparameter in training machine learning models.\n\nIt d...",
          "Type: all_summaries.docx<br>Text: It increases efficiency by reducing the frequency of weight adjustments, working in principle somewh...",
          "Type: all_summaries.docx<br>Text: 187. Day 5 - The Four Steps in LLM Training- From Forward Pass to Optimization\n\nIntroduction and Bac...",
          "Type: all_summaries.docx<br>Text: 2. __Loss Calculation:__ This step evaluates the disparity between the network's prediction of the n...",
          "Type: all_summaries.docx<br>Text: An essential clarification is made that this training process always occurs with mini-batches and it...",
          "Type: all_summaries.docx<br>Text: 189. Day 5 - Understanding Softmax and Cross-Entropy Loss in Model Training\n\nUnderstanding the Model...",
          "Type: all_summaries.docx<br>Text: While these strategies bring in a variety, they are not always necessary; picking the token with the...",
          "Type: all_summaries.docx<br>Text: Classification is especially useful for projects involving a wide range of possibilities or buckets ...",
          "Type: all_summaries.docx<br>Text: The content talks about a project primarily focused on providing an understanding of Agentic AI and ...",
          "Type: all_summaries.docx<br>Text: 5. Autonomy - This signifies that the AI system has an existence that transcends a conversation with...",
          "Type: all_summaries.docx<br>Text: A Scanner Agent, worked on previously, collects feeds, calls the GPT4 model for descriptions and pri...",
          "Type: all_summaries.docx<br>Text: I mean, to give you something that comes to mind right away is that when I was looking on hugging fa...",
          "Type: 140. Day 3 - Feature Engineering and Bag of Words- Building ML Baselines for NLP.docx<br>Text: Summary\n\nIntroduction \n\nStarting off with training session 6.3, the focus of today is on foundationa...",
          "Type: 140. Day 3 - Feature Engineering and Bag of Words- Building ML Baselines for NLP.docx<br>Text: The Business Problem \n\nIn the specific context of predicting product prices, the question of which m...",
          "Type: 140. Day 3 - Feature Engineering and Bag of Words- Building ML Baselines for NLP.docx<br>Text: 4. **Random forests** - This method combines multiple models and averages across many data and featu...",
          "Type: 141. Day 3 - Baseline Models in ML- Implementing Simple Prediction Functions.docx<br>Text: Summary\n\nThe content provides an in-depth exploration and application of baseline models in the cont...",
          "Type: 141. Day 3 - Baseline Models in ML- Implementing Simple Prediction Functions.docx<br>Text: The author demonstrates the use of the platform, explaining how to run cells, how to load data (e.g....",
          "Type: 141. Day 3 - Baseline Models in ML- Implementing Simple Prediction Functions.docx<br>Text: The guide served as a precursor to examining real machine learning models in the upcoming sessions. ...",
          "Type: AISera_2024.pdf<br>Text: LLM Evaluation: Key Metrics and Best\nPractices\nArtificial intelligence technology has yielded\nexcept...",
          "Type: AISera_2024.pdf<br>Text: Introduction to LLM Evaluation\nWhat is LLM Evaluation?\nLLM Evaluation Metrics\nEvaluation Templates\nA...",
          "Type: AISera_2024.pdf<br>Text: Overview of LLMs\nIn the current landscape, the application of large language models is significantly...",
          "Type: AISera_2024.pdf<br>Text: Primarily, it provides a window into the model’s reliability and efficiency—key factors\ndetermining ...",
          "Type: AISera_2024.pdf<br>Text: When deploying LLMs in education, for instance, developers meticulously examine the age-\nappropriate...",
          "Type: AISera_2024.pdf<br>Text: applications community feedback\nMinimization of\nToxicity\nVital for all public-facing\napplications\nTo...",
          "Type: AISera_2024.pdf<br>Text: Evaluation Templates\nYou can choose a variety of prompt templates for evaluating your fine-tuned lar...",
          "Type: AISera_2024.pdf<br>Text: Performance Assessment\nIn assessing the performance of LLMs, a range of metrics are utilized to unde...",
          "Type: AISera_2024.pdf<br>Text: Performance\nIndicator Metric Application in LLM Evaluation\nAccuracy Task Success\nRate\nMeasuring the ...",
          "Type: AISera_2024.pdf<br>Text: LLM Model Evals Versus LLM System Evals\nUnderstanding the nuances between LLM evaluations and LLM sy...",
          "Type: AISera_2024.pdf<br>Text: improving LLM evaluation methods.\nFoundational model builders consistently push the frontiers of wha...",
          "Type: AISera_2024.pdf<br>Text: Upon completing these steps, you’ll have a thorough understanding of how LLMs perform\nunder a variet...",
          "Type: AISera_2024.pdf<br>Text: Multiple LLM evaluation metrics\nIn the pursuit of LLM evaluation best practices, deploying a diversi...",
          "Type: AISera_2024.pdf<br>Text: Gain exceptional user experience & reduce support cost.\nFor those eager to witness the transformativ...",
          "Type: AISera_2024.pdf<br>Text: Custom Gen AI Demo\nProducts\nAiseraGPT\nAI Copilot\nAI Search\nAgent Assist\nAI Voice Bot\nGen AI Platform...",
          "Type: AISera_2024.pdf<br>Text: Platform\nConversational AI\nAI Workflows\nAI Customer Intelligence\nAI Support Intelligence\nAI SQL Dial...",
          "Type: AISera_2024.pdf<br>Text: Company\nPartners\nAisera Gen AI Academy\nNews Coverage\nPress Releases\nCareers\nContact Us\nTrust\nTRAPS F...",
          "Type: AISera_2024.pdf<br>Text: Let’s Work Together\nGet answers and a customized\nquote for your projects\nSubmit RFP\nSitemap | 2024 A..."
         ],
         "type": "scatter",
         "x": {
          "bdata": "AE4+QnKknEJgCS7DF7AfQWLNusKoLyTD1Chyw9MVqUImSY/Bah7KwipbgsI0bLDBQKizwQBOPkJypJxCYAkuwxewH0FizbrCqC8kw9QocsPTFalCJkmPwWoeysIqW4LCNGywwSXeeUNhgFRDSS5TQ2/WKENnVItDTPdCQ1EtbkPzUT1Dj31eQ8m0iENQrphCP7/hQphkq0IePCJD3RAWQzCYD0NZ/olDXjCXQ+lVzEJjfAxDyQQpQzioC0MJEatCgVeVQn9rYEPlFRTBbkkRQeMWPsPsB1fDVCZmw5IniMOw70HDCutDQCyc9cK4IbfCRY5AQnJRA0Exv1pCH6QNw/cobsItkPXCLHDOwhr2MUEtjzXDtHkEwkfuoMEzVyLDSc+ewpBuw8JDLoHCQPhswtV36sKn6lrDGtBJw2lngcN8C3XD",
          "dtype": "f4"
         },
         "y": {
          "bdata": "IyQlQmKrs0KDsVnD29jYwvihJkMnJf/CTZ7hwp8pIsOvmptCpSNWw0Z5gcOztz7DaNqTwyMkJUJiq7NCg7FZw9vY2ML4oSZDJyX/wk2e4cKfKSLDr5qbQqUjVsNGeYHDs7c+w6gzDMLrCCbCLsPTQaS/F0JPwSdDf/mHQpssn0I4nQFDiWgHQ643IkHgv0PBOmAFwRu2ZMJmtGzClm6xwj+Gy0IrdYFCz+PJQpA0c0PffmJDsW5RQyLDJUNFJhpDwxlCQ23kosIPxV1DLXt2Q/q5DUOqwypDKQMFQ5oQ9EKmHYvDNEf4Qmdig8NpP5jDyqiQwzgFc8MzAm3D1BUMQeMtg8GdJFXClgfdQZD2GcKUefvBh2NIwqsKvsBbzjpC1HdQwinZSMFQgLTCLyLLQc3yikLw0E1CwnGLQbnDEEJrJorA",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "height": 600,
        "margin": {
         "b": 10,
         "l": 10,
         "r": 20,
         "t": 40
        },
        "scene": {
         "xaxis": {
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "title": {
           "text": "y"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "2D Chroma Vector Store Visualization"
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name=\"GPT\"):\n",
    "    # Default llm\n",
    "    llm = ChatOpenAI(temperature=0.7, model_name=OPENAI_MODEL)\n",
    "    if model_name==\"Claude\":\n",
    "        llm = ChatAnthropic(\n",
    "            temperature=0.7,\n",
    "            model=CLAUDE_MODEL,  # or \"claude-3-opus-20240229\" for the larger model\n",
    "            anthropic_api_key=os.environ['ANTHROPIC_API_KEY']  # Set this or use environment variable ANTHROPIC_API_KEY\n",
    "        )\n",
    "    elif model_name==\"GPT\":\n",
    "        llm = ChatOpenAI(temperature=0.7, model_name=OPENAI_MODEL)\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f3/8gs03snx5mgb4bxn_w3fxrqw0000gp/T/ipykernel_82781/3183950975.py:5: LangChainDeprecationWarning:\n",
      "\n",
      "Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization is a method that reduces the precision of model parameters while maintaining the same number of parameters, which significantly decreases memory consumption. By lowering the precision from 32-bit floating point numbers to as low as 4-bit, the model can fit into limited memory without drastically affecting its performance. This approach allows larger models to be utilized in environments with restricted resources, although there is typically a slight drop in quality.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you describe quantization in a few sentences\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Can you describe quantization in a few sentences',\n",
       " 'chat_history': [HumanMessage(content='Can you describe quantization in a few sentences', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Quantization is a method that reduces the precision of model parameters while maintaining the same number of parameters, which significantly decreases memory consumption. By lowering the precision from 32-bit floating point numbers to as low as 4-bit, the model can fit into limited memory without drastically affecting its performance. This approach allows larger models to be utilized in environments with restricted resources, although there is typically a slight drop in quality.', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'Quantization is a method that reduces the precision of model parameters while maintaining the same number of parameters, which significantly decreases memory consumption. By lowering the precision from 32-bit floating point numbers to as low as 4-bit, the model can fit into limited memory without drastically affecting its performance. This approach allows larger models to be utilized in environments with restricted resources, although there is typically a slight drop in quality.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now modifying the code to include the source information of the text data used in responce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f3/8gs03snx5mgb4bxn_w3fxrqw0000gp/T/ipykernel_82781/2559153961.py:31: LangChainDeprecationWarning:\n",
      "\n",
      "The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization is a technique used to optimize machine learning models by reducing the precision of the parameters while maintaining the same number of them. For example, converting parameters from 32-bit floating-point numbers to as low as 4-bit can significantly decrease memory consumption without drastically affecting the model's performance, although there might be a slight drop in quality (source: udemy_summarizer, all_summaries.docx).\n",
      "\n",
      "An epoch in the context of machine learning training refers to one complete pass of the entire dataset through the model. Multiple epochs allow the model to refine its accuracy by making incremental improvements with each iteration of the data, which can enhance overall performance (source: udemy_summarizer, all_summaries.docx). \n",
      "\n",
      "Source Information:\n",
      "- doc_folder: udemy_summarizer\n",
      "- doc_name: all_summaries.docx\n",
      "- file_type: .docx\n",
      "- source: /Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/udemy_summarizer/all_summaries.docx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your template with explicit mention of metadata\n",
    "template = \"\"\"Answer the question based on the following context and include relevant source information:\n",
    "\n",
    "Context with Sources: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide your answer along with the sources used:\"\"\"\n",
    "\n",
    "def format_docs_with_metadata(docs):\n",
    "    \"\"\"Format documents and their metadata into a readable string\"\"\"\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        # Explicitly format metadata to be more readable\n",
    "        metadata_str = \"\\n\".join([f\"{k}: {v}\" for k, v in doc.metadata.items()])\n",
    "        formatted_doc = (\n",
    "            f\"\\n---\\nContent: {doc.page_content}\\n\"\n",
    "            f\"Source Information:\\n{metadata_str}\\n\"\n",
    "            f\"Keep all the Source Information and values of these keys:\" + \"\\n\".join([f\"{k}\" for k, v in doc.metadata.items()]) + \"\\n---\"\n",
    "        )\n",
    "        formatted_docs.append(formatted_doc)\n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "def get_context(input_dict):\n",
    "    # You might want to increase k for more results\n",
    "    docs = retriever.get_relevant_documents(input_dict[\"question\"])\n",
    "    return format_docs_with_metadata(docs)\n",
    "\n",
    "conversation_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: get_context(x),\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "query = \"Can you describe quantization and epoch definition in a few sentences\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will bring this up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain & LangChain handles the history/memory\n",
    "\n",
    "# def chat(message, history):\n",
    "    # result = conversation_chain.invoke({\"question\": message})\n",
    "    # return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain \n",
    "# & LangChain handles the history/memory\n",
    "def chat(message, history):\n",
    "    \"\"\"Chat function for Gradio interface\"\"\"\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    # result is already a string, no need to access result[\"answer\"]\n",
    "    return result\n",
    "\n",
    "#### And in Gradio:\n",
    "# Create the Gradio interface\n",
    "iface = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    type=\"messages\",\n",
    "    flagging_mode=\"manual\",\n",
    "    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
    "    save_history=True,\n",
    "    title=\"RAG-powered Chat Interface\",\n",
    "    description=\"Ask questions about the documents in the knowledge base.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dropdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Update the chatbot function based on the selected model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model_dropdown.change(select_model, inputs=model_dropdown, outputs=None)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m select_model(\u001b[43mmodel_dropdown\u001b[49m)\n\u001b[1;32m      5\u001b[0m conversation_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m     {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: get_context(x),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dropdown' is not defined"
     ]
    }
   ],
   "source": [
    "# Update the chatbot function based on the selected model\n",
    "# model_dropdown.change(select_model, inputs=model_dropdown, outputs=None)\n",
    "llm = select_model(model_dropdown)\n",
    "\n",
    "conversation_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: get_context(x),\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_steram(message, history):\n",
    "    \"\"\"Chat function for Gradio interface\"\"\"\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    for i in range(len(result)):\n",
    "        time.sleep(0.05)\n",
    "        yield result[: i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/blocks.py\", line 2088, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/blocks.py\", line 1647, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/utils.py\", line 833, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/chat_interface.py\", line 898, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/utils.py\", line 722, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/utils.py\", line 705, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/f3/8gs03snx5mgb4bxn_w3fxrqw0000gp/T/ipykernel_65068/821365022.py\", line 3, in chat_steram\n",
      "    result = conversation_chain.invoke({\"question\": message})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2994, in invoke\n",
      "    coerce_to_runnable(other),\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/langchain_core/beta/runnables/context.py\", line 153, in config_with_context\n",
      "    return _config_with_context(config, steps, _setter, _getter, threading.Event)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/langchain_core/beta/runnables/context.py\", line 76, in _config_with_context\n",
      "    context_specs = [\n",
      "                    ^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/langchain_core/beta/runnables/context.py\", line 79, in <listcomp>\n",
      "    for spec in step.config_specs\n",
      "                ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4488, in config_specs\n",
      "    objects = []\n",
      "                 \n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/functools.py\", line 1001, in __get__\n",
      "    val = self.func(instance)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4471, in deps\n",
      "    # This allows pydantic to resolve type annotations appropriately.\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: unhashable type: 'Dependency'\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    model_dropdown = gr.Dropdown(\n",
    "        [\"GPT\", \"Claude\", \"Model 3\"], label=\"Select Model\", value=\"GPT\"\n",
    "    )\n",
    "    \n",
    "    # llm = select_model(model_dropdown)\n",
    "    llm = model_dropdown.change(select_model, inputs=model_dropdown, outputs=None)\n",
    "    # llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "    # set up the conversation memory for the chat\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "    # the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "    # conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "    conversation_chain = (\n",
    "    {\n",
    "            \"context\": lambda x: get_context(x),\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "\n",
    "    chatbot = gr.ChatInterface(\n",
    "        chat_steram,\n",
    "        type=\"messages\",\n",
    "        flagging_mode=\"manual\",\n",
    "        flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
    "        save_history=True,\n",
    "    )\n",
    "\n",
    "    # Update the chatbot function based on the selected model\n",
    "    # model_dropdown.change(select_model, inputs=model_dropdown, outputs=None)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", lines=10, value=python_hard)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\")\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "def slow_echo(message, history):\n",
    "    for i in range(len(message)):\n",
    "        time.sleep(0.05)\n",
    "        yield \"You typed: \" + message[: i + 1]\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    slow_echo,\n",
    "    type=\"messages\",\n",
    "    flagging_mode=\"manual\",\n",
    "    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
    "    save_history=True,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/blocks.py:1821: UserWarning:\n",
      "\n",
      "A function (select_model) returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n",
      "    Output components:\n",
      "        []\n",
      "    Output values returned:\n",
      "        [\"Claude\"]\n",
      "\n",
      "/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/blocks.py:1821: UserWarning:\n",
      "\n",
      "A function (select_model) returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n",
      "    Output components:\n",
      "        []\n",
      "    Output values returned:\n",
      "        [\"GPT\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "def slow_echo(message, history):\n",
    "    for i in range(len(message)):\n",
    "        time.sleep(0.05)\n",
    "        yield \"You typed: \" + message[: i + 1]\n",
    "\n",
    "def select_model(model_name):\n",
    "    # This function can be used to select the model based on the dropdown value\n",
    "    # For now, it just returns the selected model name\n",
    "    return model_name\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    model_dropdown = gr.Dropdown(\n",
    "        [\"GPT\", \"Claude\", \"Model 3\"], label=\"Select Model\", value=\"GPT\"\n",
    "    )\n",
    "    chatbot = gr.ChatInterface(\n",
    "        slow_echo,\n",
    "        type=\"messages\",\n",
    "        flagging_mode=\"manual\",\n",
    "        flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
    "        save_history=True,\n",
    "    )\n",
    "\n",
    "    # Update the chatbot function based on the selected model\n",
    "    model_dropdown.change(select_model, inputs=model_dropdown, outputs=None)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javadmollakazemi/PycharmProjects/15_llm_engineering/llm_engineering/.conda/lib/python3.11/site-packages/gradio/components/chatbot.py:288: UserWarning:\n",
      "\n",
      "The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatAnthropic(\n",
    "    temperature=0.7,\n",
    "    model=CLAUDE_MODEL\n",
    ")\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"  # Specify which output to store in memory\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# Create the conversation chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define the chat function for Gradio\n",
    "def chat(message, history):\n",
    "    result = conversation_chain({\"question\": message})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    title=\"RAG-powered Chat Interface\",\n",
    "    description=\"Ask questions about the documents in the knowledge base.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
